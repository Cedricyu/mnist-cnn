{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "FfgXBwuzjPdB"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "MbS__i5fbk4D"
      },
      "outputs": [],
      "source": [
        "class ImageLoader:\n",
        "    def __init__(self):\n",
        "        self.num_images = 0\n",
        "        self.images = None\n",
        "        self.labels = None\n",
        "        self.mnist = fetch_openml('mnist_784', version=1)\n",
        "\n",
        "    def load_data(self, num_images=5000):\n",
        "        self.num_images = num_images\n",
        "        mnist = self.mnist\n",
        "        X, y = mnist['data'], mnist['target'].astype(int)\n",
        "        X_array = X.values  # Convert DataFrame to NumPy array\n",
        "        self.images = X_array[:self.num_images].reshape(self.num_images, 28, 28)  # Reshape images to 2D\n",
        "        self.labels = y[:self.num_images]  # Get the corresponding labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "gZcw21dKjdia"
      },
      "outputs": [],
      "source": [
        "class ConvLayer:\n",
        "    def __init__(self, num_filters, filter_size=3):\n",
        "        self.num_filters = num_filters\n",
        "        self.filter_size = filter_size\n",
        "        self.filters = np.random.randn(num_filters, filter_size, filter_size) * 0.01\n",
        "        self.filter_grads = np.zeros_like(self.filters)  # To store gradients\n",
        "        self.output_height = None\n",
        "        self.output_width = None\n",
        "\n",
        "    def conv2d(self, input_data):\n",
        "        num_filters, filter_size = self.num_filters, self.filter_size\n",
        "        num_input_filters, height, width = input_data.shape\n",
        "        self.output_height = max(0, height - filter_size + 1)  # Save output dimensions\n",
        "        self.output_width = max(0, width - filter_size + 1)\n",
        "        output = np.zeros((num_filters, self.output_height, self.output_width))\n",
        "\n",
        "        for f in range(num_filters):\n",
        "            filter = self.filters[f]\n",
        "            for i in range(self.output_height):\n",
        "                for j in range(self.output_width):\n",
        "                    region = input_data[:, i:i+filter_size, j:j+filter_size]\n",
        "                    output[f, i, j] = np.sum(region * filter)\n",
        "\n",
        "        return output\n",
        "\n",
        "    def max_pooling(self, feature_maps, pool_size=2, stride=2):\n",
        "        num_filters, height, width = feature_maps.shape\n",
        "        self.output_height = (height - pool_size) // stride + 1  # Save output dimensions\n",
        "        self.output_width = (width - pool_size) // stride + 1\n",
        "        output = np.zeros((num_filters, self.output_height, self.output_width))\n",
        "\n",
        "        for f in range(num_filters):\n",
        "            for i in range(0, self.output_height * stride, stride):\n",
        "                for j in range(0, self.output_width * stride, stride):\n",
        "                    if i + pool_size <= height and j + pool_size <= width:\n",
        "                        region = feature_maps[f, i:i+pool_size, j:j+pool_size]\n",
        "                        output[f, i//stride, j//stride] = np.max(region)\n",
        "\n",
        "        return output\n",
        "\n",
        "    def update_filters(self, learning_rate=0.01):\n",
        "        self.filters -= learning_rate * self.filter_grads\n",
        "        self.filter_grads = np.zeros_like(self.filters)  # Reset gradients\n",
        "\n",
        "    def compute_gradients(self, input_data, grad_output):\n",
        "        self.filter_grads = np.zeros_like(self.filters)\n",
        "        filter_size = self.filters.shape[1]  # Get filter size from the current layer\n",
        "\n",
        "        for f in range(self.num_filters):\n",
        "            for i in range(self.output_height):\n",
        "                for j in range(self.output_width):\n",
        "                    # Ensure input_data slice matches the number of filters in the previous layer\n",
        "                    self.filter_grads[f] += np.sum(input_data[:, i:i+filter_size, j:j+filter_size] * grad_output[f, i, j], axis=0)\n",
        "\n",
        "        # print(\"Filter Gradients:\", self.filter_grads)  # Print the calculated filter gradients\n",
        "        return self.filter_grads\n",
        "\n",
        "\n",
        "    def backward(self, input_data, grad_output):\n",
        "        num_input_filters, height, width = input_data.shape\n",
        "        num_filters, filter_size = self.num_filters, self.filter_size\n",
        "        d_input = np.zeros_like(input_data)  # Gradient with respect to input data\n",
        "        self.filter_grads = np.zeros_like(self.filters)  # Reset filter gradients\n",
        "\n",
        "        # print(\"Input Data Shape:\", input_data.shape)\n",
        "        # print(\"Gradient Output Shape:\", grad_output.shape)\n",
        "        # print(\"Filter Shape:\", self.filters.shape)\n",
        "\n",
        "        # Calculate gradients with respect to filters\n",
        "        for f in range(num_filters):\n",
        "            for i in range(self.output_height):\n",
        "                for j in range(self.output_width):\n",
        "                    region = input_data[:, i:i+filter_size, j:j+filter_size]\n",
        "                    self.filter_grads[f] += np.sum(\n",
        "                        region * grad_output[f, i, j], axis=0\n",
        "                    )\n",
        "\n",
        "                    # # Debug messages for filter gradients\n",
        "                    # print(f\"Filter Grad {f} Shape:\", self.filter_grads[f].shape)\n",
        "                    # print(f\"Region Shape: {region.shape}\")\n",
        "                    # print(f\"Grad Output [{f}, {i}, {j}] Shape: {grad_output[f, i, j].shape}\")\n",
        "\n",
        "        # Calculate gradients with respect to input data\n",
        "        for f in range(num_filters):\n",
        "            for i in range(self.output_height):\n",
        "                # print(f\"Filter {f} Shape:\", self.filters[f].shape)\n",
        "                for j in range(self.output_width):\n",
        "                    region = input_data[:, i:i+filter_size, j:j+filter_size]\n",
        "                    d_input[:, i:i+filter_size, j:j+filter_size] += self.filters[f] * grad_output[f, i, j]\n",
        "\n",
        "        #             print(f\"Grad Output [{f}, {i}, {j}] Shape: {grad_output[f, i, j].shape}\")\n",
        "        #             print(f\"d_input Region Shape: {d_input[:, i:i+filter_size, j:j+filter_size].shape}\")\n",
        "\n",
        "        # print(\"Computed Gradients with Respect to Input:\", d_input)\n",
        "        return d_input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "xkSu4YiFmw0c"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def display_images(original_image, first_layer_output, second_layer_output):\n",
        "    # Calculate number of rows for subplots based on number of filters\n",
        "    num_rows_first = first_layer_output.shape[0] // 2 + (first_layer_output.shape[0] % 2)\n",
        "    num_rows_second = second_layer_output.shape[0] // 3 + (second_layer_output.shape[0] % 3)\n",
        "\n",
        "    # Create a figure with enough subplots\n",
        "    fig = plt.figure(figsize=(15, 5 + 3 * (num_rows_first + num_rows_second)))\n",
        "\n",
        "    # Original Image\n",
        "    plt.subplot(num_rows_first + num_rows_second + 1, 3, 1)\n",
        "    plt.imshow(original_image, cmap='gray')\n",
        "    plt.title('Original Image')\n",
        "    plt.axis('off')\n",
        "\n",
        "    # First Layer Outputs\n",
        "    for i in range(first_layer_output.shape[0]):\n",
        "        plt.subplot(num_rows_first + num_rows_second + 1, 3, 3 + i + 1)\n",
        "        plt.imshow(first_layer_output[i], cmap='gray')\n",
        "        plt.title(f'First Layer Filter {i+1}')\n",
        "        plt.axis('off')\n",
        "\n",
        "    # Second Layer Outputs\n",
        "    for i in range(second_layer_output.shape[0]):\n",
        "        plt.subplot(num_rows_first + num_rows_second + 1, 3, 3 + first_layer_output.shape[0] + i + 1)\n",
        "        plt.imshow(second_layer_output[i], cmap='gray')\n",
        "        plt.title(f'Second Layer Filter {i+1}')\n",
        "        plt.axis('off')\n",
        "\n",
        "    plt.tight_layout()  # Adjust layout to prevent overlapping\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "spLB8xoonAuu",
        "outputId": "9ee8abe5-71a7-482f-fd50-ff3a893bd5a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/datasets/_openml.py:1022: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
            "  warn(\n"
          ]
        }
      ],
      "source": [
        "loader = ImageLoader()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "sq80m_m1tzB4"
      },
      "outputs": [],
      "source": [
        "class SimpleNN:\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        self.W1 = np.random.randn(input_size, hidden_size) * 0.01\n",
        "        self.b1 = np.zeros((1, hidden_size))\n",
        "        self.W2 = np.random.randn(hidden_size, output_size) * 0.01\n",
        "        self.b2 = np.zeros((1, output_size))\n",
        "\n",
        "    def relu(self, x):\n",
        "        return np.maximum(0, x)\n",
        "\n",
        "    def softmax(self, x):\n",
        "        exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
        "        return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.z1 = np.dot(x, self.W1) + self.b1\n",
        "        self.a1 = self.relu(self.z1)\n",
        "        self.z2 = np.dot(self.a1, self.W2) + self.b2\n",
        "        self.a2 = self.softmax(self.z2)\n",
        "        return self.a2\n",
        "\n",
        "    def compute_loss(self, predictions, targets):\n",
        "        m = targets.shape[0]\n",
        "        log_likelihood = -np.log(predictions[range(m), targets])\n",
        "        loss = np.sum(log_likelihood) / m\n",
        "        return loss\n",
        "\n",
        "    def backward(self, x, predictions, targets):\n",
        "        m = x.shape[0]  # Number of samples\n",
        "\n",
        "        # Gradient of the loss with respect to the output of the final layer\n",
        "        d_z2 = predictions\n",
        "        d_z2[range(m), targets] -= 1\n",
        "        d_z2 /= m\n",
        "\n",
        "        # Gradients for weights and biases of the final layer\n",
        "        d_W2 = np.dot(self.a1.T, d_z2)\n",
        "        d_b2 = np.sum(d_z2, axis=0, keepdims=True)\n",
        "\n",
        "        # Gradient with respect to activations of the first layer\n",
        "        d_a1 = np.dot(d_z2, self.W2.T)\n",
        "        d_z1 = d_a1 * (self.a1 > 0)  # ReLU derivative\n",
        "\n",
        "        # Gradients for weights and biases of the first layer\n",
        "        d_W1 = np.dot(x.T, d_z1)\n",
        "        d_b1 = np.sum(d_z1, axis=0, keepdims=True)\n",
        "\n",
        "        # Update weights and biases\n",
        "        self.W1 -= 0.01 * d_W1\n",
        "        self.b1 -= 0.01 * d_b1\n",
        "        self.W2 -= 0.01 * d_W2\n",
        "        self.b2 -= 0.01 * d_b2\n",
        "\n",
        "        # Return gradient with respect to the input\n",
        "        return np.dot(d_z1, self.W1.T)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "TWBKPU7kxI2C"
      },
      "outputs": [],
      "source": [
        "loader.load_data(num_images=10000)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "NgU5BPMtMiix"
      },
      "outputs": [],
      "source": [
        "conv1 = ConvLayer(num_filters=4)\n",
        "conv2 = ConvLayer(num_filters=9)\n",
        "\n",
        "X_images_train, X_images_test, y_train, y_test = train_test_split(loader.images, loader.labels, test_size=0.1, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "ScOsCXhmNAwT"
      },
      "outputs": [],
      "source": [
        "input_size = 225\n",
        "hidden_size = 128\n",
        "# Calculate the number of unique classes in the training labels\n",
        "output_size = 10\n",
        "\n",
        "nn = SimpleNN(input_size, hidden_size, output_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "0bwFS4kQMDly"
      },
      "outputs": [],
      "source": [
        "def train_nn_with_conv(nn, X_images_train, y_train, epochs=10, learning_rate=0.01):\n",
        "\n",
        "    # Prepare the transformed training data\n",
        "    X_train_transformed = []\n",
        "    for original_image in X_images_train:\n",
        "        # Apply first convolution and pooling layer\n",
        "        first_layer_output = conv1.conv2d(original_image[np.newaxis, :, :])\n",
        "        first_layer_output = conv1.max_pooling(first_layer_output)\n",
        "\n",
        "        # Apply second convolution and pooling layer\n",
        "        second_layer_output = conv2.conv2d(first_layer_output)\n",
        "        second_layer_output = conv2.max_pooling(second_layer_output)\n",
        "\n",
        "        # Flatten the output and store it\n",
        "        X_train_transformed.append(second_layer_output.flatten())\n",
        "    X_train_transformed = np.array(X_train_transformed)\n",
        "    y_train = np.array(y_train)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        # Training loop\n",
        "        for i in range(len(X_train_transformed)):\n",
        "\n",
        "            flattened_output = X_train_transformed[i]\n",
        "            label = y_train[i]\n",
        "\n",
        "            # Forward pass\n",
        "            predictions = nn.forward(flattened_output[np.newaxis, :])\n",
        "            # print(type(predictions))  # Print the type of predictions to verify\n",
        "\n",
        "            # Compute loss and gradients\n",
        "            loss = nn.compute_loss(predictions, np.array([label]))\n",
        "            d_flattened_output = nn.backward(flattened_output[np.newaxis, :], predictions, np.array([label]))\n",
        "\n",
        "            # Reshape gradient for the second ConvLayer\n",
        "            # second_layer_output_shape = (conv2.num_filters, conv2.output_height, conv2.output_width)\n",
        "            # second_layer_grad = d_flattened_output.reshape(second_layer_output_shape)\n",
        "\n",
        "            # # Compute gradients for ConvLayer 2\n",
        "            # conv2_grads = conv2.compute_gradients(first_layer_output, second_layer_grad)\n",
        "            # conv2.update_filters(learning_rate)\n",
        "\n",
        "            # # Compute gradients for ConvLayer 1\n",
        "            # first_layer_grad = conv2.backward(first_layer_output, second_layer_grad)\n",
        "            # conv1_grads = conv1.compute_gradients(original_image[np.newaxis, :, :], first_layer_grad)\n",
        "            # conv1.update_filters(learning_rate)\n",
        "\n",
        "            total_loss += loss\n",
        "\n",
        "        average_loss = total_loss / len(X_train_transformed)\n",
        "        print(f'Epoch {epoch + 1}/{epochs}, Loss: {average_loss}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sDoM47A4Nq_a",
        "outputId": "85af3342-ae8a-4701-fedd-dc852e83bf79"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Loss: 0.918558462614493\n",
            "Epoch 2/10, Loss: 0.511554604392777\n",
            "Epoch 3/10, Loss: 0.40776368218077785\n",
            "Epoch 4/10, Loss: 0.35019233377741193\n",
            "Epoch 5/10, Loss: 0.3114340401628969\n",
            "Epoch 6/10, Loss: 0.2825193152294618\n",
            "Epoch 7/10, Loss: 0.2588123790038082\n",
            "Epoch 8/10, Loss: 0.23886365145916857\n",
            "Epoch 9/10, Loss: 0.22135102297253176\n",
            "Epoch 10/10, Loss: 0.2059497538134155\n"
          ]
        }
      ],
      "source": [
        "train_nn_with_conv(nn, X_images_train, y_train, epochs=10, learning_rate=0.01)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "Wopcv0XR8mcw"
      },
      "outputs": [],
      "source": [
        "def test_nn(nn, X_test, y_test):\n",
        "    X_test_transformed = []\n",
        "    y_test = np.array(y_test)\n",
        "\n",
        "    for original_image in X_test:\n",
        "        # Apply first convolution and pooling layer\n",
        "        first_layer_output = conv1.conv2d(original_image[np.newaxis, :, :])\n",
        "        first_layer_output = conv1.max_pooling(first_layer_output)\n",
        "\n",
        "        # Apply second convolution and pooling layer\n",
        "        second_layer_output = conv2.conv2d(first_layer_output)\n",
        "        second_layer_output = conv2.max_pooling(second_layer_output)\n",
        "\n",
        "        # Flatten the output and store it\n",
        "        X_test_transformed.append(second_layer_output.flatten())\n",
        "\n",
        "    X_test_transformed = np.array(X_test_transformed)\n",
        "    correct_predictions = 0\n",
        "    total_samples = len(X_test)\n",
        "\n",
        "    y_pred = []\n",
        "\n",
        "    for i in range(total_samples):\n",
        "        x = X_test_transformed[i]\n",
        "        # Forward pass\n",
        "        predictions = nn.forward(x[np.newaxis, :])\n",
        "\n",
        "        # Convert predictions to class labels\n",
        "        predicted_class = np.argmax(predictions)\n",
        "        y_pred.append(predicted_class)\n",
        "\n",
        "    report = classification_report(y_test, y_pred, digits=4)\n",
        "    print(\"Classification Report:\\n\", report)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_nn(nn, X_images_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DU-ECSeh6HAM",
        "outputId": "482e5c81-3c53-46de-b026-50076579141d"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9266    0.9712    0.9484       104\n",
            "           1     0.9554    0.9907    0.9727       108\n",
            "           2     0.9175    0.9468    0.9319        94\n",
            "           3     0.8421    0.8000    0.8205       100\n",
            "           4     0.9158    0.8614    0.8878       101\n",
            "           5     0.9551    0.8947    0.9239        95\n",
            "           6     0.9725    0.9217    0.9464       115\n",
            "           7     0.9048    0.9223    0.9135       103\n",
            "           8     0.9176    0.9512    0.9341        82\n",
            "           9     0.8462    0.8980    0.8713        98\n",
            "\n",
            "    accuracy                         0.9160      1000\n",
            "   macro avg     0.9153    0.9158    0.9151      1000\n",
            "weighted avg     0.9164    0.9160    0.9157      1000\n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}