{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report"
      ],
      "metadata": {
        "id": "FfgXBwuzjPdB"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ImageLoader:\n",
        "    def __init__(self):\n",
        "        self.num_images = 0\n",
        "        self.images = None\n",
        "        self.labels = None\n",
        "        self.mnist = fetch_openml('mnist_784', version=1)\n",
        "\n",
        "    def load_data(self, num_images=5000):\n",
        "        self.num_images = num_images\n",
        "        mnist = self.mnist\n",
        "        X, y = mnist['data'], mnist['target'].astype(int)\n",
        "        X_array = X.values  # Convert DataFrame to NumPy array\n",
        "        self.images = X_array[:self.num_images].reshape(self.num_images, 28, 28)  # Reshape images to 2D\n",
        "        self.labels = y[:self.num_images]  # Get the corresponding labels"
      ],
      "metadata": {
        "id": "MbS__i5fbk4D"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ConvLayer:\n",
        "    def __init__(self, num_filters, filter_size=3):\n",
        "        self.num_filters = num_filters\n",
        "        self.filter_size = filter_size\n",
        "        self.filters = np.random.randn(num_filters, filter_size, filter_size) * 0.01\n",
        "        self.filter_grads = np.zeros_like(self.filters)  # To store gradients\n",
        "        self.output_height = None\n",
        "        self.output_width = None\n",
        "\n",
        "    def conv2d(self, input_data):\n",
        "        num_filters, filter_size = self.num_filters, self.filter_size\n",
        "        num_input_filters, height, width = input_data.shape\n",
        "        self.output_height = max(0, height - filter_size + 1)  # Save output dimensions\n",
        "        self.output_width = max(0, width - filter_size + 1)\n",
        "        output = np.zeros((num_filters, self.output_height, self.output_width))\n",
        "\n",
        "        for f in range(num_filters):\n",
        "            filter = self.filters[f]\n",
        "            for i in range(self.output_height):\n",
        "                for j in range(self.output_width):\n",
        "                    region = input_data[:, i:i+filter_size, j:j+filter_size]\n",
        "                    output[f, i, j] = np.sum(region * filter)\n",
        "\n",
        "        return output\n",
        "\n",
        "    def max_pooling(self, feature_maps, pool_size=2, stride=2):\n",
        "        num_filters, height, width = feature_maps.shape\n",
        "        self.output_height = (height - pool_size) // stride + 1  # Save output dimensions\n",
        "        self.output_width = (width - pool_size) // stride + 1\n",
        "        output = np.zeros((num_filters, self.output_height, self.output_width))\n",
        "\n",
        "        for f in range(num_filters):\n",
        "            for i in range(0, self.output_height * stride, stride):\n",
        "                for j in range(0, self.output_width * stride, stride):\n",
        "                    if i + pool_size <= height and j + pool_size <= width:\n",
        "                        region = feature_maps[f, i:i+pool_size, j:j+pool_size]\n",
        "                        output[f, i//stride, j//stride] = np.max(region)\n",
        "\n",
        "        return output\n",
        "\n",
        "    def update_filters(self, learning_rate=0.01):\n",
        "        self.filters -= learning_rate * self.filter_grads\n",
        "        self.filter_grads = np.zeros_like(self.filters)  # Reset gradients\n",
        "\n",
        "    def compute_gradients(self, input_data, grad_output):\n",
        "        self.filter_grads = np.zeros_like(self.filters)\n",
        "        filter_size = self.filters.shape[1]  # Get filter size from the current layer\n",
        "\n",
        "        for f in range(self.num_filters):\n",
        "            for i in range(self.output_height):\n",
        "                for j in range(self.output_width):\n",
        "                    # Ensure input_data slice matches the number of filters in the previous layer\n",
        "                    self.filter_grads[f] += np.sum(input_data[:, i:i+filter_size, j:j+filter_size] * grad_output[f, i, j], axis=0)\n",
        "\n",
        "        # print(\"Filter Gradients:\", self.filter_grads)  # Print the calculated filter gradients\n",
        "        return self.filter_grads\n",
        "\n",
        "\n",
        "    def backward(self, input_data, grad_output):\n",
        "        num_input_filters, height, width = input_data.shape\n",
        "        num_filters, filter_size = self.num_filters, self.filter_size\n",
        "        d_input = np.zeros_like(input_data)  # Gradient with respect to input data\n",
        "        self.filter_grads = np.zeros_like(self.filters)  # Reset filter gradients\n",
        "\n",
        "        # print(\"Input Data Shape:\", input_data.shape)\n",
        "        # print(\"Gradient Output Shape:\", grad_output.shape)\n",
        "        # print(\"Filter Shape:\", self.filters.shape)\n",
        "\n",
        "        # Calculate gradients with respect to filters\n",
        "        for f in range(num_filters):\n",
        "            for i in range(self.output_height):\n",
        "                for j in range(self.output_width):\n",
        "                    region = input_data[:, i:i+filter_size, j:j+filter_size]\n",
        "                    self.filter_grads[f] += np.sum(\n",
        "                        region * grad_output[f, i, j], axis=0\n",
        "                    )\n",
        "\n",
        "                    # # Debug messages for filter gradients\n",
        "                    # print(f\"Filter Grad {f} Shape:\", self.filter_grads[f].shape)\n",
        "                    # print(f\"Region Shape: {region.shape}\")\n",
        "                    # print(f\"Grad Output [{f}, {i}, {j}] Shape: {grad_output[f, i, j].shape}\")\n",
        "\n",
        "        # Calculate gradients with respect to input data\n",
        "        for f in range(num_filters):\n",
        "            for i in range(self.output_height):\n",
        "                # print(f\"Filter {f} Shape:\", self.filters[f].shape)\n",
        "                for j in range(self.output_width):\n",
        "                    region = input_data[:, i:i+filter_size, j:j+filter_size]\n",
        "                    d_input[:, i:i+filter_size, j:j+filter_size] += self.filters[f] * grad_output[f, i, j]\n",
        "\n",
        "        #             print(f\"Grad Output [{f}, {i}, {j}] Shape: {grad_output[f, i, j].shape}\")\n",
        "        #             print(f\"d_input Region Shape: {d_input[:, i:i+filter_size, j:j+filter_size].shape}\")\n",
        "\n",
        "        # print(\"Computed Gradients with Respect to Input:\", d_input)\n",
        "        return d_input"
      ],
      "metadata": {
        "id": "gZcw21dKjdia"
      },
      "execution_count": 165,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def display_images(original_image, first_layer_output, second_layer_output):\n",
        "    # Calculate number of rows for subplots based on number of filters\n",
        "    num_rows_first = first_layer_output.shape[0] // 2 + (first_layer_output.shape[0] % 2)\n",
        "    num_rows_second = second_layer_output.shape[0] // 3 + (second_layer_output.shape[0] % 3)\n",
        "\n",
        "    # Create a figure with enough subplots\n",
        "    fig = plt.figure(figsize=(15, 5 + 3 * (num_rows_first + num_rows_second)))\n",
        "\n",
        "    # Original Image\n",
        "    plt.subplot(num_rows_first + num_rows_second + 1, 3, 1)\n",
        "    plt.imshow(original_image, cmap='gray')\n",
        "    plt.title('Original Image')\n",
        "    plt.axis('off')\n",
        "\n",
        "    # First Layer Outputs\n",
        "    for i in range(first_layer_output.shape[0]):\n",
        "        plt.subplot(num_rows_first + num_rows_second + 1, 3, 3 + i + 1)\n",
        "        plt.imshow(first_layer_output[i], cmap='gray')\n",
        "        plt.title(f'First Layer Filter {i+1}')\n",
        "        plt.axis('off')\n",
        "\n",
        "    # Second Layer Outputs\n",
        "    for i in range(second_layer_output.shape[0]):\n",
        "        plt.subplot(num_rows_first + num_rows_second + 1, 3, 3 + first_layer_output.shape[0] + i + 1)\n",
        "        plt.imshow(second_layer_output[i], cmap='gray')\n",
        "        plt.title(f'Second Layer Filter {i+1}')\n",
        "        plt.axis('off')\n",
        "\n",
        "    plt.tight_layout()  # Adjust layout to prevent overlapping\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "xkSu4YiFmw0c"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loader = ImageLoader()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "spLB8xoonAuu",
        "outputId": "af2bf98f-ae1e-42d4-fa4c-d5a84b9a82c2"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/datasets/_openml.py:1022: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
            "  warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleNN:\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        self.W1 = np.random.randn(input_size, hidden_size) * 0.01\n",
        "        self.b1 = np.zeros((1, hidden_size))\n",
        "        self.W2 = np.random.randn(hidden_size, output_size) * 0.01\n",
        "        self.b2 = np.zeros((1, output_size))\n",
        "\n",
        "    def relu(self, x):\n",
        "        return np.maximum(0, x)\n",
        "\n",
        "    def softmax(self, x):\n",
        "        exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
        "        return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.z1 = np.dot(x, self.W1) + self.b1\n",
        "        self.a1 = self.relu(self.z1)\n",
        "        self.z2 = np.dot(self.a1, self.W2) + self.b2\n",
        "        self.a2 = self.softmax(self.z2)\n",
        "        return self.a2\n",
        "\n",
        "    def compute_loss(self, predictions, targets):\n",
        "        m = targets.shape[0]\n",
        "        log_likelihood = -np.log(predictions[range(m), targets])\n",
        "        loss = np.sum(log_likelihood) / m\n",
        "        return loss\n",
        "\n",
        "    def backward(self, x, predictions, targets):\n",
        "        m = x.shape[0]  # Number of samples\n",
        "\n",
        "        # Gradient of the loss with respect to the output of the final layer\n",
        "        d_z2 = predictions\n",
        "        d_z2[range(m), targets] -= 1\n",
        "        d_z2 /= m\n",
        "\n",
        "        # Gradients for weights and biases of the final layer\n",
        "        d_W2 = np.dot(self.a1.T, d_z2)\n",
        "        d_b2 = np.sum(d_z2, axis=0, keepdims=True)\n",
        "\n",
        "        # Gradient with respect to activations of the first layer\n",
        "        d_a1 = np.dot(d_z2, self.W2.T)\n",
        "        d_z1 = d_a1 * (self.a1 > 0)  # ReLU derivative\n",
        "\n",
        "        # Gradients for weights and biases of the first layer\n",
        "        d_W1 = np.dot(x.T, d_z1)\n",
        "        d_b1 = np.sum(d_z1, axis=0, keepdims=True)\n",
        "\n",
        "        # Update weights and biases\n",
        "        self.W1 -= 0.01 * d_W1\n",
        "        self.b1 -= 0.01 * d_b1\n",
        "        self.W2 -= 0.01 * d_W2\n",
        "        self.b2 -= 0.01 * d_b2\n",
        "\n",
        "        # Return gradient with respect to the input\n",
        "        return np.dot(d_z1, self.W1.T)\n",
        "\n"
      ],
      "metadata": {
        "id": "sq80m_m1tzB4"
      },
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def flatten_feature_maps(feature_maps):\n",
        "    num_filters, height, width = feature_maps.shape\n",
        "    return feature_maps.reshape(num_filters * height * width)\n",
        "\n",
        "\n",
        "def predict(cnn, X):\n",
        "    # Handle the case where X is already flattened\n",
        "    if X.ndim == 2:\n",
        "        predictions = [nn.forward(x[np.newaxis, :]) for x in X]\n",
        "    else:\n",
        "        predictions = [nn.forward(x[np.newaxis, :, :]) for x in X]\n",
        "    return np.argmax(np.vstack(predictions), axis=1)\n",
        "\n",
        "def accuracy(y_true, y_pred):\n",
        "    return np.mean(y_true == y_pred)"
      ],
      "metadata": {
        "id": "ACdL8keHI1Xk"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loader.load_data(num_images=10000)\n"
      ],
      "metadata": {
        "id": "TWBKPU7kxI2C"
      },
      "execution_count": 174,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conv1 = ConvLayer(num_filters=4)\n",
        "conv2 = ConvLayer(num_filters=9)\n",
        "\n",
        "# Prepare training data\n",
        "X_train = []\n",
        "y_train = []\n",
        "\n",
        "for i in range(loader.num_images):\n",
        "    original_image = loader.images[i]\n",
        "    label = loader.labels[i]\n",
        "\n",
        "    first_layer_output = conv1.conv2d(original_image[np.newaxis, :, :])\n",
        "    first_layer_output = conv1.max_pooling(first_layer_output)\n",
        "\n",
        "    second_layer_output = conv2.conv2d(first_layer_output)\n",
        "    second_layer_output = conv2.max_pooling(second_layer_output)\n",
        "\n",
        "    X_train.append(second_layer_output)\n",
        "    y_train.append(label)\n",
        "\n",
        "X_train = np.array(X_train)\n",
        "y_train = np.array(y_train)"
      ],
      "metadata": {
        "id": "NgU5BPMtMiix"
      },
      "execution_count": 175,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_size = 225\n",
        "hidden_size = 128\n",
        "# Calculate the number of unique classes in the training labels\n",
        "output_size = 10\n",
        "\n",
        "nn = SimpleNN(input_size, hidden_size, output_size)"
      ],
      "metadata": {
        "id": "ScOsCXhmNAwT"
      },
      "execution_count": 167,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_nn_with_conv(nn, X_train, y_train, epochs=10, learning_rate=0.01):\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        for i in range(len(X_train)):\n",
        "            flattened_output = flatten_feature_maps(X_train[i])\n",
        "            label = y_train[i]\n",
        "\n",
        "            # Forward pass\n",
        "            predictions = nn.forward(flattened_output[np.newaxis, :])\n",
        "\n",
        "            # Compute loss and gradients\n",
        "            loss = nn.compute_loss(predictions, np.array([label]))\n",
        "            d_flattened_output = nn.backward(flattened_output[np.newaxis, :], predictions, np.array([label]))\n",
        "\n",
        "            second_layer_output_shape = (conv2.num_filters, conv2.output_height, conv2.output_width)\n",
        "            second_layer_grad = d_flattened_output.reshape(second_layer_output_shape)\n",
        "\n",
        "            # Compute gradients for ConvLayer\n",
        "            conv2_grads = conv2.compute_gradients(first_layer_output, second_layer_grad)\n",
        "            conv2.update_filters(learning_rate)\n",
        "\n",
        "            # Compute gradients for the first ConvLayer\n",
        "            first_layer_grad = conv2.backward(first_layer_output, second_layer_grad)\n",
        "            conv1_grads = conv1.compute_gradients(original_image[np.newaxis, :, :], first_layer_grad)\n",
        "            conv1.update_filters(learning_rate)\n",
        "\n",
        "            total_loss += loss\n",
        "\n",
        "            # print('d_flattened_output_shape :',d_flattened_output.shape)\n",
        "\n",
        "        print(f'Epoch {epoch + 1}/{epochs}, Loss: {total_loss / len(X_train)}')"
      ],
      "metadata": {
        "id": "0bwFS4kQMDly"
      },
      "execution_count": 168,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_nn_with_conv(nn, X_train, y_train, epochs=10, learning_rate=0.01)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sDoM47A4Nq_a",
        "outputId": "3f60c73a-a5b3-4a81-fa32-b2c65b9a0337"
      },
      "execution_count": 176,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Loss: 0.5768630866078364\n",
            "Epoch 2/10, Loss: 0.36347822219080583\n",
            "Epoch 3/10, Loss: 0.30012252933896816\n",
            "Epoch 4/10, Loss: 0.25791541576778015\n",
            "Epoch 5/10, Loss: 0.22789794834524604\n",
            "Epoch 6/10, Loss: 0.20403406026544246\n",
            "Epoch 7/10, Loss: 0.18537302412447373\n",
            "Epoch 8/10, Loss: 0.16971274886756946\n",
            "Epoch 9/10, Loss: 0.15695023172019046\n",
            "Epoch 10/10, Loss: 0.14595960439535066\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def test_nn_with_random_subset(nn, X_train, y_train, num_samples=1000):\n",
        "    # Randomly select indices from the training data\n",
        "    indices = np.random.choice(len(X_train), num_samples, replace=False)\n",
        "\n",
        "    # Create a random subset of the training data\n",
        "    X_subset = X_train[indices]\n",
        "    y_subset = y_train[indices]\n",
        "\n",
        "    correct_predictions = 0\n",
        "    total_samples = num_samples\n",
        "\n",
        "    for i in range(total_samples):\n",
        "        # Get the input and target label\n",
        "        x = X_subset[i]\n",
        "        y_true = y_subset[i]\n",
        "\n",
        "        x = flatten_feature_maps(x)\n",
        "        # Forward pass\n",
        "        predictions = nn.forward(x[np.newaxis, :])\n",
        "\n",
        "        # Convert predictions to class labels\n",
        "        predicted_class = np.argmax(predictions)\n",
        "\n",
        "        # Compare with true label\n",
        "        if predicted_class == y_true:\n",
        "            correct_predictions += 1\n",
        "\n",
        "    accuracy = correct_predictions / total_samples\n",
        "    print(f\"Test Accuracy on Random Subset: {accuracy * 100:.2f}%\")\n",
        "\n",
        "# Assuming `nn` is your neural network model, `X_train` is your training input data, and `y_train` is your training labels\n",
        "test_nn_with_random_subset(nn, X_train, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wopcv0XR8mcw",
        "outputId": "ee107781-a592-4d90-d7c9-8817dc6d2346"
      },
      "execution_count": 177,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy on Random Subset: 95.70%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(conv1.filters, conv2.filters)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0KP-04VYP_PJ",
        "outputId": "4afc2cbd-38bf-4181-e8e8-a49a89458eb4"
      },
      "execution_count": 179,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[[-0.00533629  0.00276472 -0.00598206]\n",
            "  [ 0.00620088 -0.00499089  0.00648803]\n",
            "  [-0.01431477  0.00910912  0.01205135]]\n",
            "\n",
            " [[ 0.00836681 -0.00377282  0.03214547]\n",
            "  [-0.00257807  0.00652423  0.00011743]\n",
            "  [ 0.00237463 -0.00516728  0.00316406]]\n",
            "\n",
            " [[-0.00776258  0.00686992  0.00097155]\n",
            "  [-0.00316623 -0.01503088  0.02035842]\n",
            "  [ 0.00184129  0.00127207 -0.00165518]]\n",
            "\n",
            " [[ 0.007705    0.00160803  0.00147585]\n",
            "  [ 0.01189663 -0.01955609  0.0017215 ]\n",
            "  [-0.00420174 -0.00106368 -0.00150472]]] [[[-4.09691254e+01 -7.98003319e+01 -8.54250997e+01]\n",
            "  [-8.28698396e+01 -1.13366964e+02 -1.28935174e+02]\n",
            "  [-9.31884154e+01 -1.30916108e+02 -1.37050016e+02]]\n",
            "\n",
            " [[ 6.75392619e+00  4.61338684e+00  4.94504003e+00]\n",
            "  [ 8.78022529e+00  1.88547137e+00  4.99226905e+00]\n",
            "  [-4.41378276e+00 -2.12129851e+01 -3.04094476e+01]]\n",
            "\n",
            " [[ 7.78117647e+00  1.95113430e+01  4.53247116e+01]\n",
            "  [ 2.21243633e+01  4.16464688e+01  8.21300215e+01]\n",
            "  [ 2.54165714e+01  4.62836207e+01  7.53026283e+01]]\n",
            "\n",
            " [[ 1.13115607e+00  9.11946841e+00  3.09541193e+01]\n",
            "  [-4.00306463e-01  1.26875061e+01  3.88966783e+01]\n",
            "  [-1.17564372e+01  1.09265072e+01  3.53866350e+01]]\n",
            "\n",
            " [[ 9.22319371e+00  2.84859761e+01  4.40957425e+01]\n",
            "  [ 3.39908662e+01  4.69329294e+01  7.55276869e+01]\n",
            "  [ 4.95070344e+01  4.57345344e+01  6.71742573e+01]]\n",
            "\n",
            " [[ 1.81075789e+01  6.20909689e+01  1.23317660e+02]\n",
            "  [ 4.09372114e+01  1.02879971e+02  1.71478631e+02]\n",
            "  [ 2.98590725e+01  8.79609412e+01  1.70876630e+02]]\n",
            "\n",
            " [[-1.51555103e+01 -4.58751072e+01 -9.25184153e+01]\n",
            "  [-3.59230723e+01 -8.98518800e+01 -1.45118555e+02]\n",
            "  [-3.45451934e+01 -9.73383256e+01 -1.58687338e+02]]\n",
            "\n",
            " [[ 6.65955391e+01  1.52242746e+02  2.32889944e+02]\n",
            "  [ 1.54127456e+02  3.07490352e+02  4.18179194e+02]\n",
            "  [ 2.11292600e+02  4.00658400e+02  5.12552527e+02]]\n",
            "\n",
            " [[-1.03764934e+01 -2.34809549e+01 -2.63075101e+01]\n",
            "  [-2.45054510e+01 -3.61443409e+01 -3.78384625e+01]\n",
            "  [-2.73791556e+01 -4.76166803e+01 -4.48431884e+01]]]\n"
          ]
        }
      ]
    }
  ]
}